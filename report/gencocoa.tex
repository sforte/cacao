\section{Generalized CoCoA}

\subsection{Setup}
As presented in the CoCoA paper \cite{CoCoA} the algorithm currently has support for problems having the following
primal form:
\sdcaPrimal

It is though possible, in a very similar theoretical and implementative framework, to solve problems of the following
more general form:
\gensdcaPrimal
where $\primalreg$ is a 1-convex function with respect to the L2 norm.

Similarly as for the L2 regularized problem solved by CoCoA, and as done in \cite{ShalevShwartz:2012tn}, we can define
a dual problem:
\gensdcaDual
where:
\vwdef
A primal-dual relation also holds for these two problems, with the duality gap \dualitygap acting as an upperbound
to the suboptimality of the primal and the dual.

\subsection{Method description}
In this section we present a generalized CoCoA algorithm to solve this more general problem. The high level structure of
the algorithm is very similar (if not almost identical) to the original algorithm and it is as follows: \\
\gencocoa
The only difference to the original CoCoA is the use of the $\vv(\av)$ vectors instead of the $\wv(\av)$. This vector
is computed at the end, using the gradient of the dual conjugate of the regularizer. Another difference is that the
local method is now required to optimize the generalized dual form as in \ref{eq:gensdcaDual} (for its local coordinated)
instead of the original CoCoA problem.
Same as in \cite{CoCoA} we'll also make the following assumption on the local solver:
\localgeomimpr
where:
\suboptimality
Given this we can state an theorem equivalent to Theorem 2 in \cite{CoCoA} (the proof can be found in the appendix):
\convergence

\subsection{Local SDCA}
In this section we'll present an SDCA method, very similar to the one in \cite{ShalevShwartz:2012tn} used to solve the
generalized local dual problem. The algorithm is as follows and it's also very similar to the CoCoA one: \\
\genlocalsdca

Unfortunately solving the problem:
$$\schardproblem$$
can be very hard for complex regularizers and it's replaced in \cite{ShalevShwartz:2012tn} by the following relaxed
lowerbound problem (which still allows for finding the exact solution):
$$\screlaxedproblem$$
that is much simpler to solve since it only requires us to be able to compute the gradient $\nabla \dualreg$ and after
that it can easily be solved by line search. It is also possible to notice that this lowerbound problem is actually
strict in the case of L2 regularization.

\subsubsection{Single coordinate optimizer}
For the sake of modularity, the second line in the SDCA loop has been abstracted in the code with the concept of
single coordinate optimizer and a couple of them have been implemented; the 3 main ones are:
\begin{itemize}
\item BrentMethodOptimizer: this optimizer uses a derivative free line search method called Brent's method to solve the
single coordinate problem. This allows from great generality since it easily allows to construct a solver for a new
 loss function by simply pluggin in the conjugate of the loss function. More common and maybe fast method will possibly
 be explored later on, but it's unlikely they will give any benefit since this optimization is not the bottle neck of
 the algorithm.
\item Ad-hoc solver for SVM: this is basically the same as in the standard CoCoA paper. It is worth noticing that replacing
this solver with the brent's method solver does not bring any noticeable loss of precision.
\item Ad-hoc solver for Ridge regression: similarly as for svm, the ridge regression single coordinate problem is solvable
in closed form.
\end{itemize}

\subsubsection{Supported regularizers}
In addition to L2 regularization elastic net has also been implemented and it works really well, provided that the L1 term
is not more than three orders of magnitude smaller than the L1 term.
An approximation scheme for the L1, taken from
\cite{ShalevShwartz:2012tn} has also been implemented (it consists in setting the L2 term to a very small value);
unfortunately from some recent experiments it seems that it does not perform quite as well as liblinear (on logistic regression).
In any case it still seems kind of practical.

Any combination of this losses and regularizers is possible.

\subsubsection{Passcode}
Another local solver provided as an option is passcode wild, taken from the Passcode paper (arxiv: 1504.01365v1). The
algorithm consists of a parallel version of SDCA where $t$ threads work in parallel on the same $w$ vector (kept in shared
memory) with no synchronization whatsoever (in the sense that they read and write from it with no locking or synchronization).
This can result in losing the relationship between the $\alpha$ and the $w$ (but I'm reconstructing the $w$ from scratch
from the alpha once in a while) but it gives a basically linear speedup. The correctness of the method in the paper
has only been proven for L2 regularized losses but from my experiments it works just as fine for elastic net (even with
a very small L2 term).
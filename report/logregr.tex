\section{CoCoA for Logistic Regression}
The initial solver has been restructured to be more general and allow different loss functions, local solvers and losses. The main structure of the algorithm stays pretty much the same but there now exist the possibility to specify what local solver to use, allowing to choose also fully primal solvers; the only local solver currently implemented is SDCA.

\subsection{SDCA}
The SDCA algorithm is the same as it was in the original CoCoA repository but an abstraction has been created to allow for custom "Single coordinate optimizers" (SCO). What the SCO does is, given a point $(x,y)$, the old $\alpha$ value and the old $w$ vector, is solving the following problem:
$$ 
	- \frac{\lambda n}{2}  \norm{ \wv^{(h-1)} +\frac{1}{\lambda n} \Delta\alpha\,\x_i }^2 
	- \ell_i^*\big(-(\alpha_i^{(h-1)}+\Delta\alpha)\big)
$$
and returning a better value for $\Delta\alpha$ (non necessarily the optimum). \\
It is also possible, and implemented in the library, to solve the primal SCO directly, optimizing on $\wv_i$ :
$$
\frac1{n} \ell_i( (\overline\wv+\wv_i)^T \x_i )  + \frac{\lambda}{2}\norm{\wv_i}^2.
$$
This problem can be obtained by looking at the local primal problem as defined in the CoCoA paper and assume that we have two partitions where one is made by point $\x_i$ only and the other by all the other points. We can also rewrite it as:
$$
\frac1{n} \ell_i( (\wv^{(h-1)}-\wv^{(h-1)}_i+\wv_i)^T \x_i )  + \frac{\lambda}{2}\norm{\wv_i}^2
$$
But since $\wv_i^{(h-1)}$ and $\wv_i$ need to be a linear combination of the vectors in the partition, they thus have to be a scaling of $\x_i$, respectively $\alpha^{(h-1)}\x_i$ and $\alpha\x_i$, giving us:
$$
\frac1{n} \ell_i\big( (\wv^{(h-1)})^T\x_i - \alpha^{(h-1)}\norm{\x_i}^2/(\lambda n) + \alpha\norm{\x_i}^2/(\lambda n) \big)  + \frac{\lambda}{2}\norm{(\alpha^{(h-1)} +\Delta\alpha)\x_i/(\lambda n)}^2
$$
finally getting
$$ 
	 \ell_i\Big(\x_i^T \wv + \Delta\alpha \norm{x}^2/(\lambda n) \Big)
	+ \frac{(\alpha^{(h-1)} + \Delta\alpha)^2}{\lambda n}\norm{\x_i}^2.
$$
Solving this problem (with respect to $\Delta\alpha$) has several advantages over solving the dual form:
\begin{itemize}
\item It allows to solve the problem without the need of a pen-and-paper derivation of the convex conjugate.
\item It requires no explicit constraint over the domain of the $\alpha$ variables, which is instead needed for the the dual form. This allows to use solvers such as SGD or Newton's method, instead of requiring more complex bracketing based methods such as Brent's method.
\end{itemize}
 
\subsection{Logistic regression}
The logistic regression problem can be solved using different methods for either the primal or the dual single-coordinate problem:
\begin{itemize}
\item BrentOptimizer for the dual SC problem: it uses Brent's method from the apache-commons library to minimize a custom loss function; in our case the dual SC problem. The method does not require any derivative being plugged in. The exact form of the convex conjugate loss used is:
$$
\ell^*_y(\alpha) = (1+\alpha y)\log{(1+\alpha y)} - \alpha y \log{(-\alpha y)}
$$	
having domain equal to $(-1,0)$ for $y = 1$ and $(0,1)$ for $y = -1$.

\item BrentOptimizer for the primal SC problem: same as before but it now solves the primal SC problem. The loss is of course:
$$
\ell_y(p) = \log{(1+\exp{(-py)})}.
$$

\item Newton's method for the primal SC problem: being the primal SCP unconstrained we can use Newton's method, feeding to it the first and the second derivative of the primal SCP. This two are constructed using the first and second derivative of $\ell_y$:
$$
\ell'_y(p) = \frac{-y}{1+\exp{(py)}}
$$
and
$$
\ell''_y(p) = \frac{y \exp(py)}{(1+\exp{(py)})^2}.
$$
The derivatives of $\ell_i\Big(\x_i^T \wv + \Delta\alpha \norm{x}^2/(\lambda n) \Big)$ with respect to $\Delta\alpha$, needed to construct the primal $SCP$ to be fed to Newton's, are then computed in the code using the chain rule. This allows for a cleaner and simpler definition of new loss functions (and their derivatives).

\end{itemize}